{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ddatta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ddatta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import networkx as nx\n",
    "# import utils.preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import stellargraph as sg \n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "import multiprocessing\n",
    "from joblib import Parallel,delayed\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "import collections\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = './processed_data/DBLP/'\n",
    "\n",
    "nodes_author_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        src_dir,\n",
    "        'nodes_author.csv'),\n",
    "    index_col = 0\n",
    ")\n",
    "nodes_paper_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        src_dir,\n",
    "        'nodes_paper.csv'),\n",
    "    index_col = 0\n",
    ")\n",
    "nodes_term_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        src_dir,\n",
    "        'nodes_term.csv'),\n",
    "    index_col = 0\n",
    ")\n",
    "nodes_conf_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        src_dir,\n",
    "        'nodes_conf.csv'),\n",
    "    index_col = 0\n",
    ")\n",
    "\n",
    "\n",
    "fpath_list = ['PT_edges.csv','PC_edges.csv','PA_edges.csv']\n",
    "df_edges = None\n",
    "for fpath in fpath_list:\n",
    "    _df = pd.read_csv( os.path.join(src_dir,fpath), index_col = None )\n",
    "    if df_edges is None : df_edges = _df\n",
    "    else:\n",
    "        df_edges = df_edges.append(_df,ignore_index= True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_obj = StellarGraph({\n",
    "    \"author\": nodes_author_df, \n",
    "    \"paper\":nodes_paper_df,\n",
    "    \"term\": nodes_term_df,\n",
    "    \"conf\": nodes_conf_df\n",
    "},\n",
    "    df_edges\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph details ::  StellarGraph: Undirected multigraph\n",
      " Nodes: 26128, Edges: 119783\n",
      "\n",
      " Node types:\n",
      "  paper: [14328]\n",
      "    Features: none\n",
      "    Edge types: paper-default->author, paper-default->conf, paper-default->term\n",
      "  term: [7723]\n",
      "    Features: none\n",
      "    Edge types: term-default->paper\n",
      "  author: [4057]\n",
      "    Features: none\n",
      "    Edge types: author-default->paper\n",
      "  conf: [20]\n",
      "    Features: none\n",
      "    Edge types: conf-default->paper\n",
      "\n",
      " Edge types:\n",
      "    paper-default->term: [85810]\n",
      "        Weights: all 1 (default)\n",
      "    paper-default->author: [19645]\n",
      "        Weights: all 1 (default)\n",
      "    paper-default->conf: [14328]\n",
      "        Weights: all 1 (default)\n"
     ]
    }
   ],
   "source": [
    "print('Graph details :: ', graph_obj.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ========================== \n",
      "Number of random walks: 209024\n"
     ]
    }
   ],
   "source": [
    "def generate_random_walks(graph_obj , num_walks_per_node, walk_length):\n",
    "    random_walk_object = BiasedRandomWalk(graph_obj)\n",
    "    cpu_count = multiprocessing.cpu_count()\n",
    "    res =  Parallel(n_jobs=8)(delayed(aux_gen_walks)(graph_obj, walk_length, random_walk_object) for i in range(num_walks_per_node))\n",
    "    all_walks = []\n",
    "    for r in res:\n",
    "        all_walks.extend(r)\n",
    "    return all_walks\n",
    "\n",
    "def aux_gen_walks(\n",
    "    graph_obj, \n",
    "    walk_length, \n",
    "    random_walk_object,\n",
    "    num_walks = 1,\n",
    "    p = 0.5, \n",
    "    q = 2\n",
    "):\n",
    "    walks = random_walk_object.run(\n",
    "        nodes=graph_obj.nodes(),\n",
    "        length=walk_length,\n",
    "        n=num_walks,\n",
    "        p=p, \n",
    "        q=q,\n",
    "        weighted=False,\n",
    "        seed=np.random.randint(100)\n",
    "    )\n",
    "    return walks\n",
    "\n",
    "# ================================================================= # \n",
    "print(\" ========================== \")\n",
    "\n",
    "walk_length = 64\n",
    "num_walks_per_node = 15\n",
    "emb_dim\n",
    "\n",
    "\n",
    "walks_save_file = \"random_walks_{}_{}.npy\".format(walk_length, num_walks_per_node)\n",
    "try:\n",
    "    walks_np_arr = np.load( walks_save_file )\n",
    "    walks = [ list(_) for _ in walks_np_arr]\n",
    "except:\n",
    "    walks = generate_random_walks(graph_obj, num_walks_per_node, walk_length)\n",
    "    walks_np_arr = np.array(walks)\n",
    "    np.save( walks_save_file, walks_np_arr)\n",
    "\n",
    "    \n",
    "print(\"Number of random walks: {}\".format(len(walks)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_walks = [[str(n) for n in walk] for walk in walks]\n",
    "n2v_model = Word2Vec(\n",
    "    str_walks, size=emb_dim, window=5, min_count=0, sg=1, workers=-1, iter=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
