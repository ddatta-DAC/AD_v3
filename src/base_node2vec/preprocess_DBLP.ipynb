{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import networkx as nx\n",
    "# import utils.preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "author_label = pd.read_csv('dblp/data/raw/DBLP/author_label.txt', sep='\\t', header=None, names=['author_id', 'label', 'author_name'], keep_default_na=False, encoding='utf-8')\n",
    "paper_author = pd.read_csv('dblp/data/raw/DBLP/paper_author.txt', sep='\\t', header=None, names=['paper_id', 'author_id'], keep_default_na=False, encoding='utf-8')\n",
    "paper_conf = pd.read_csv('dblp/data/raw/DBLP/paper_conf.txt', sep='\\t', header=None, names=['paper_id', 'conf_id'], keep_default_na=False, encoding='utf-8')\n",
    "paper_term = pd.read_csv('dblp/data/raw/DBLP/paper_term.txt', sep='\\t', header=None, names=['paper_id', 'term_id'], keep_default_na=False, encoding='utf-8')\n",
    "papers = pd.read_csv('dblp/data/raw/DBLP/paper.txt', sep='\\t', header=None, names=['paper_id', 'paper_title'], keep_default_na=False, encoding='cp1252')\n",
    "terms = pd.read_csv('dblp/data/raw/DBLP/term.txt', sep='\\t', header=None, names=['term_id', 'term'], keep_default_na=False, encoding='utf-8')\n",
    "confs = pd.read_csv('dblp/data/raw/DBLP/conf.txt', sep='\\t', header=None, names=['conf_id', 'conf'], keep_default_na=False, encoding='utf-8')\n",
    "authors = author_label['author_id'].to_list()\n",
    "paper_author = paper_author[paper_author['author_id'].isin(authors)].reset_index(drop=True)\n",
    "valid_papers = paper_author['paper_id'].unique()\n",
    "print('Number of papers :', len(valid_papers))\n",
    "\n",
    "papers = papers[papers['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "paper_conf = paper_conf[paper_conf['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "print('Number of papers :', len(paper_conf))\n",
    "\n",
    "paper_term = paper_term[paper_term['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "valid_terms = paper_term['term_id'].unique()\n",
    "terms = terms[terms['term_id'].isin(valid_terms)].reset_index(drop=True)\n",
    "\n",
    "# term lemmatization and grouping\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_id_mapping = {}\n",
    "lemma_list = []\n",
    "lemma_id_list = []\n",
    "i = 0\n",
    "for _, row in terms.iterrows():\n",
    "    i += 1\n",
    "    lemma = lemmatizer.lemmatize(row['term'])\n",
    "    lemma_list.append(lemma)\n",
    "    if lemma not in lemma_id_mapping:\n",
    "        lemma_id_mapping[lemma] = row['term_id']\n",
    "    lemma_id_list.append(lemma_id_mapping[lemma])\n",
    "terms['lemma'] = lemma_list\n",
    "terms['lemma_id'] = lemma_id_list\n",
    "\n",
    "term_lemma_mapping = {row['term_id']: row['lemma_id'] for _, row in terms.iterrows()}\n",
    "lemma_id_list = []\n",
    "for _, row in paper_term.iterrows():\n",
    "    lemma_id_list.append(term_lemma_mapping[row['term_id']])\n",
    "paper_term['lemma_id'] = lemma_id_list\n",
    "\n",
    "paper_term = paper_term[['paper_id', 'lemma_id']]\n",
    "paper_term.columns = ['paper_id', 'term_id']\n",
    "paper_term = paper_term.drop_duplicates()\n",
    "terms = terms[['lemma_id', 'lemma']]\n",
    "terms.columns = ['term_id', 'term']\n",
    "terms = terms.drop_duplicates()\n",
    "\n",
    "# filter out stopwords from terms\n",
    "stopwords = sklearn_stopwords.union(set(nltk_stopwords.words('english')))\n",
    "stopword_id_list = terms[terms['term'].isin(stopwords)]['term_id'].to_list()\n",
    "paper_term = paper_term[~(paper_term['term_id'].isin(stopword_id_list))].reset_index(drop=True)\n",
    "terms = terms[~(terms['term'].isin(stopwords))].reset_index(drop=True)\n",
    "len(terms)\n",
    "\n",
    "author_label = author_label.sort_values('author_id').reset_index(drop=True)\n",
    "papers = papers.sort_values('paper_id').reset_index(drop=True)\n",
    "terms = terms.sort_values('term_id').reset_index(drop=True)\n",
    "confs = confs.sort_values('conf_id').reset_index(drop=True)\n",
    "\n",
    "print('Number of conferences ', len(confs))\n",
    "print('Number of authors ', len(author_label))\n",
    "print('Number of terms ', len(terms))\n",
    "print('Number of papers ', len(papers))\n",
    "\n",
    "authors_list = list(author_label['author_id'])\n",
    "papers_list = list(papers['paper_id'])\n",
    "term_list = list(terms['term_id'])\n",
    "conf_list = list(confs['conf_id'])\n",
    "dim = len(authors_list) + len(papers_list) + len(term_list) + len(confs)\n",
    "print(' Total entities', dim)\n",
    "\n",
    "\n",
    "author_id_mapping = {row['author_id']: i for i, row in author_label.iterrows()}\n",
    "paper_id_mapping = {row['paper_id']: i + len(author_label) for i, row in papers.iterrows()}\n",
    "term_id_mapping = {row['term_id']: i + len(author_label) + len(papers) for i, row in terms.iterrows()}\n",
    "conf_id_mapping = {row['conf_id']: i + len(author_label) + len(papers) + len(terms) for i, row in confs.iterrows()}\n",
    "\n",
    "\n",
    "entity_id_map = pd.DataFrame(\n",
    "    columns=['domain', 'entity_id','serial_id']\n",
    ")\n",
    "type_dict = { 'author': author_id_mapping, 'paper': paper_id_mapping, 'term': term_id_mapping, 'conf': conf_id_mapping }\n",
    "for _type,_dict in type_dict.items():\n",
    "    i = list(_dict.keys())\n",
    "    j = list(_dict.values())\n",
    "    _df = pd.DataFrame( data = {'entity_id': i ,'serial_id': j } )\n",
    "    _df['domain'] = _type\n",
    "    entity_id_map = entity_id_map.append(_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "# --------------------- #\n",
    "data_save_path = 'processed_data/DBLP'\n",
    "if not os.path.exists('processed_data'):\n",
    "    os.mkdir('processed_data')\n",
    "if not os.path.exists(data_save_path):\n",
    "    os.mkdir(data_save_path)\n",
    "entity_id_map.to_csv( os.path.join( data_save_path, 'entity_id_mapping.csv') ) \n",
    "\n",
    "# Create graph data\n",
    "nodes_author_df = pd.DataFrame( data = { 'author' : list(author_id_mapping.values()) })\n",
    "nodes_paper_df = pd.DataFrame(  data = { 'paper' : list(paper_id_mapping.values()) } )\n",
    "nodes_term_df = pd.DataFrame( data = { 'term' : list(term_id_mapping.values()) } )\n",
    "nodes_conf_df = pd.DataFrame(  data = { 'conf' : list(conf_id_mapping.values()) } )\n",
    "\n",
    "nodes_author_df.to_csv(os.path.join(data_save_path,'nodes_author.csv'),index = False)\n",
    "nodes_paper_df.to_csv(os.path.join(data_save_path,'nodes_paper.csv'),index = False)\n",
    "nodes_term_df.to_csv(os.path.join(data_save_path,'nodes_term.csv'),index = False)\n",
    "nodes_conf_df.to_csv(os.path.join(data_save_path,'nodes_conf.csv'),index = False)\n",
    "\n",
    "PA_edge_list = []\n",
    "for _, row in paper_author.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = author_id_mapping[row['author_id']]\n",
    "    PA_edge_list.append((idx1,idx2))\n",
    "    \n",
    "df = pd.DataFrame ( data =  np.array(PA_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PA_edges.csv')\n",
    "df.to_csv(fpath, index=False)\n",
    "    \n",
    "PT_edge_list = []\n",
    "for _, row in paper_term.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = term_id_mapping[row['term_id']]\n",
    "    PT_edge_list.append((idx1,idx2))\n",
    "\n",
    "df = pd.DataFrame ( data =  np.array(PT_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PT_edges.csv')\n",
    "df.to_csv(fpath, index=False)\n",
    "    \n",
    "\n",
    "PC_edge_list = []\n",
    "for _, row in paper_conf.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = conf_id_mapping[row['conf_id']]\n",
    "    PC_edge_list.append((idx1,idx2))\n",
    "\n",
    "df = pd.DataFrame ( data = np.array(PC_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PC_edges.csv')\n",
    "df.to_csv(fpath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
